{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out \n",
    "    \n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "Resnet50 = ResNet50()\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\"./PROJECT\"), '../..')))\n",
    "from Models.Util.loadData import create_hanzi_dataloaders\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(p=0.5),\n",
    "     transforms.RandomVerticalFlip(p=0.5),\n",
    "     transforms.RandomCrop(32, padding=4, padding_mode=\"reflect\"),\n",
    "     transforms.ToTensor(),\n",
    "    #  transforms.Normalize([0.5], [0.5])\n",
    "     ])\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "def get_train_loader(batch_size, transform):\n",
    "    \n",
    "    trainloader = create_hanzi_dataloaders(root_dir=\"../../Dataset/hanzi/processed\", batch_size=batch_size,preprocess=train_transform)\n",
    "    return trainloader\n",
    "    \n",
    "    \n",
    "\n",
    "trainloader = get_train_loader(batch_size, train_transform)\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def get_optimizer(net, lr):\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def accuracy(output, target):\n",
    "    # get the index of the max log-probability\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    return pred.eq(target.view_as(pred)).float().mean()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = get_optimizer(Resnet50, 0.1)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1 / 40], batches: [100 / 147], loss: 6.817, acc: 12.91\n",
      "Epoch: [2 / 40], batches: [100 / 147], loss: 2.185, acc: 16.19\n",
      "Epoch: [3 / 40], batches: [100 / 147], loss: 2.066, acc: 20.41\n",
      "Epoch: [4 / 40], batches: [100 / 147], loss: 1.829, acc: 27.50\n",
      "Epoch: [5 / 40], batches: [100 / 147], loss: 1.663, acc: 36.50\n",
      "Epoch: [6 / 40], batches: [100 / 147], loss: 1.447, acc: 46.25\n",
      "Epoch: [7 / 40], batches: [100 / 147], loss: 1.216, acc: 54.81\n",
      "Epoch: [8 / 40], batches: [100 / 147], loss: 0.884, acc: 69.38\n",
      "Epoch: [9 / 40], batches: [100 / 147], loss: 0.740, acc: 73.91\n",
      "Epoch: [10 / 40], batches: [100 / 147], loss: 0.563, acc: 80.59\n",
      "Epoch: [11 / 40], batches: [100 / 147], loss: 0.460, acc: 84.44\n",
      "Epoch: [12 / 40], batches: [100 / 147], loss: 0.435, acc: 85.97\n",
      "Epoch: [13 / 40], batches: [100 / 147], loss: 0.407, acc: 86.22\n",
      "Epoch: [14 / 40], batches: [100 / 147], loss: 0.377, acc: 88.31\n",
      "Epoch: [15 / 40], batches: [100 / 147], loss: 0.314, acc: 89.72\n",
      "Epoch: [16 / 40], batches: [100 / 147], loss: 0.324, acc: 89.47\n",
      "Epoch: [17 / 40], batches: [100 / 147], loss: 0.284, acc: 90.56\n",
      "Epoch: [18 / 40], batches: [100 / 147], loss: 0.241, acc: 92.12\n",
      "Epoch: [19 / 40], batches: [100 / 147], loss: 0.255, acc: 91.88\n",
      "Epoch: [20 / 40], batches: [100 / 147], loss: 0.227, acc: 92.59\n",
      "Epoch: [21 / 40], batches: [100 / 147], loss: 0.209, acc: 93.75\n",
      "Epoch: [22 / 40], batches: [100 / 147], loss: 0.197, acc: 93.69\n",
      "Epoch: [23 / 40], batches: [100 / 147], loss: 0.194, acc: 93.56\n",
      "Epoch: [24 / 40], batches: [100 / 147], loss: 0.204, acc: 93.25\n",
      "Epoch: [25 / 40], batches: [100 / 147], loss: 0.175, acc: 94.25\n",
      "Epoch: [26 / 40], batches: [100 / 147], loss: 0.173, acc: 94.44\n",
      "Epoch: [27 / 40], batches: [100 / 147], loss: 0.185, acc: 94.03\n",
      "Epoch: [28 / 40], batches: [100 / 147], loss: 0.164, acc: 94.62\n",
      "Epoch: [29 / 40], batches: [100 / 147], loss: 0.155, acc: 95.28\n",
      "Epoch: [30 / 40], batches: [100 / 147], loss: 0.122, acc: 96.09\n",
      "Epoch: [31 / 40], batches: [100 / 147], loss: 0.124, acc: 96.09\n",
      "Epoch: [32 / 40], batches: [100 / 147], loss: 0.108, acc: 96.38\n",
      "Epoch: [33 / 40], batches: [100 / 147], loss: 0.126, acc: 95.88\n",
      "Epoch: [34 / 40], batches: [100 / 147], loss: 0.102, acc: 96.78\n",
      "Epoch: [35 / 40], batches: [100 / 147], loss: 0.102, acc: 97.00\n",
      "Epoch: [36 / 40], batches: [100 / 147], loss: 0.088, acc: 97.38\n",
      "Epoch: [37 / 40], batches: [100 / 147], loss: 0.089, acc: 97.00\n",
      "Epoch: [38 / 40], batches: [100 / 147], loss: 0.092, acc: 97.38\n",
      "Epoch: [39 / 40], batches: [100 / 147], loss: 0.073, acc: 97.72\n",
      "Epoch: [40 / 40], batches: [100 / 147], loss: 0.068, acc: 97.78\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def forward_step(net, inputs, labels):\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    return outputs, loss, labels\n",
    "\n",
    "def train(net, loader, optimizer, max_epoch):\n",
    "    net.to(device)\n",
    "    net.train()\n",
    "    N = len(loader)\n",
    "    print_interval = (N // 8 // 100 + 1) * 100\n",
    "    for epoch in range(max_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        for i, data in enumerate(loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            images, labels = data[0].to(device), data[1].to(device) \n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs, loss, labels = forward_step(net, images, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            running_acc += accuracy(outputs, labels).item()\n",
    "            if (i + 1) % print_interval == 0:\n",
    "                print('Epoch: [%d / %d], batches: [%d / %d], loss: %.3f, acc: %.2f' %\n",
    "                      (epoch + 1, max_epoch, i + 1, N, \n",
    "                       running_loss / print_interval, 100 * running_acc / print_interval))\n",
    "                running_loss = 0.0\n",
    "                running_acc = 0.0\n",
    "                \n",
    "        scheduler.step()\n",
    "        \n",
    "    print('Finished Training')\n",
    "\n",
    "\n",
    "train(Resnet50, trainloader, optimizer, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(Resnet50.state_dict(), './checkpoints/resnet50_ddpm.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
